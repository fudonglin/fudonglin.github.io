# Byte Pair Encoding (GPT Style Tokenizer)

## Preface

If you‚Äôre a fan of **The Flash**, you probably remember the iconic opening line:

> *‚ÄúMy name is Barry Allen, and I‚Äôm the fastest man alive.‚Äù*

You might also recall an earlier version of the Flash from Earth-2, distinguished by a winged helmet. That design choice isn‚Äôt random. It traces back to **Mercury**, the Roman god of trade and speed, traditionally depicted with winged sandals or a winged cap‚Äîsymbols of swiftness and motion.







![Mecurry](https://github.com/fudonglin/fudonglin.github.io/blob/main/_posts/2026-02-07-bpe/Mercury.jpg?raw=true)



Figure 1: Mercury, the Roman god of trade and speed.



This influence goes far beyond comic book design. Many English words associated with trade or speed can be traced back to Mercury‚Äôs name, such as:

- **Trade-related**: `merchandise`, `merchant`, and `commerce`
- **Speed-related**: `mercurial`, the planet *Mercury*

What‚Äôs striking is that all of these words share the same subword: `merc`. As we‚Äôll see later, this kind of meaningful subword structure can emerge naturally‚Äîeven without any linguistic rules‚Äîwhen we apply frequency-based tokenization algorithms like GPT‚Äòs **Byte Pair Encoding (BPE)**.



## The Role of Tokenizers

To begin with, let's understand the roles of tokenizers in large language models (LLMs).

Although LLMs such as ChatGPT have demonstrated impressive capabilities in generating and reasoning about human language, machine learning models themselves do not truly understand human language. Instead, neural networks are designed to read and process only numerical representations‚Äîor, more precisely, binary numbers.

Imagine traveling to a foreign country where you do not speak the local language: you would need a translator to communicate with local residents. Tokenizers in language models play a similar "translator" role. They convert human language into machine-readable numerical representations as input, and then translate those numerical representations back into human language as output. 

![tokenizer_role](https://github.com/fudonglin/fudonglin.github.io/blob/main/_posts/2026-02-07-bpe/tokenizer_role.png?raw=true)

Figure 2: The role of tokenizers in Transformer-based lanuage model.



Tokenizers are required at both the input and output stages. At the input stage, they encode human language (e.g., words or text) into numerical token IDs. At the output stage, they decode the generated token IDs back into human language.



Tokenizers consist of three key components: **Tokenized Text**, **Token IDs**, and **Token Embeddings**. In the encoding stage, the input text is first split into tokens (for example, words or subwords), which are then mapped to discrete token IDs. These token IDs are subsequently projected into high-dimensional continuous vectors through an embedding lookup table, producing the token embeddings consumed by the Transformers.

In the decoding stage, the process is reversed: the output embeddings of Transformers are mapped back to token IDs, which are then converted into human-readable tokenized text.

![token_lookup_table](https://github.com/fudonglin/fudonglin.github.io/blob/main/_posts/2026-02-07-bpe/token_lookup_table.png?raw=true)

Figure 3: How tokenizers map text input to Token IDs and Token Embeddings.



## Character and Word-Level Tokenizers

### Character-Level Tokenizer

When we translate human language into numerical representations, the simplest idea is to operate at the character level: assign a unique number to each character and use that number as its representation. This is exactly how Python‚Äôs built-in `ord()` function works‚Äîit maps a single character to its corresponding Unicode integer value. 

![ASCII_table](https://github.com/fudonglin/fudonglin.github.io/blob/main/_posts/2026-02-07-bpe/ASCII_table.png?raw=true)

Figure 4: SCII table used by Python for character-to-integer mapping.



For example, 97 to 122 are the ASCII number equivalent to `a` to and `z`:

```python
x, y = ord('a'), ord('z')

print(x) # Ouput 97
print(y) # Ouput 122
```



#### Limitations of Character-level Tokenizer

- **Long and Inefficient Sequences**:  Character-level tokenizer produces unnecessarily long sequences. For example, `the cat chased the mouse` becomes 24 tokens. Because Transformer complexity scales as $O(n^2)$ with sequence length, this quickly leads to high computational cost.

- **Lack of Semantic Meaning**:  Individual characters carry little semantic information. For example, the word `mouse` could refer to either an animal or a computer device. However, when `mouse` appears in the same sentence as `cat` and `chased`, it is far more likely to represent the animal. Requiring models to infer such higher-level semantics solely from raw character sequences is extremely challenging.

- **Ineffective Use of Frequency Patterns**:  Treating text purely as characters ignores natural word-frequency structure. Common words and rare words are represented uniformly, limiting the model‚Äôs ability to efficiently exploit statistical regularities in language.



### Word-Level Tokenizer

A natural improvement over character-level tokenizer is word-level tokenizer, where each unique word in the vocabulary is assigned a distinct numerical ID. Compared to character-level representations, word-level tokens are more semantically meaningful and lead to much shorter sequences. For example, the sentence `the cat chased the mouse` can be represented using only 5 tokens (ignoring the white spaces) at the word level, greatly reducing sequence length and computational cost.

| Tokenized Text | Token ID |            Word Embedding            |
| :------------: | :------: | :----------------------------------: |
|      the       |   1437   | [-0.9200, -0.1600, -0.3100, -0.1000] |
|      cat       |   5389   | [ 0.4800, -1.2000, -0.4400, 2.2100]  |
|     chased     |   7234   | [ 0.5600, -0.3500, -1.0900, -0.4300] |
|      the       |   1437   | [-0.9200, -0.1600, -0.3100, -0.1000] |
|     mouse      |   4321   |  [-0.8300, 0.0400, 0.4900, -1.0700]  |



#### Limations of Word-Level Encoding

- **Morphological Inflexibility**: Word-level encoding treats related forms like `run`, `running`, and `runner` as independent tokens, preventing effective sharing of statistical information across word variants.
- **Out-of-Vocabulary (OOV) Issues**: Fixed word vocabularies cannot handle unseen words, misspellings, or domain-specific terms, forcing them into an `unknown` token and losing semantic detail.



## Byte Pair Encoding

The limitations of both character-level and word-level tokenizer motivate a more flexible representation that lies between characters and full words. Ideally, we want a tokenizer that preserves semantic meaning, avoids out-of-vocabulary (OOV) issues, and keeps the vocabulary size manageable‚Äîwhile still producing reasonably short sequences.

**[Byte Pair Encoding (BPE)](https://aclanthology.org/P16-1162.pdf)** addresses these goals by operating at the subword level. Popularized by **[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)**, BPE has become the foundation tokenization algorithm for modern large lanuage models (LLMs). Today, virtually all state-of-the-art LLMs‚Äîincluding **GPT**, **LLaMA**, **Mistral**, and **Qwen**‚Äîrely on BPE-style algorithms or their variants for tokenization. 

Rather than treating words as indivisible units, BPE begins with individual characters and iteratively merges the most frequent adjacent character pairs into new tokens. Through this process, common words and meaningful subwords naturally emerge as single tokens, while rare or unseen words remain decomposable into smaller, known units. 



## Byte-Level Byte Pair Encoding (GPT-2 Style)

GPT-2 adopts **byte-level Byte Pair Encoding (BPE)**, which operates directly on UTF-8 bytes rather than characters or words. In the following example, we use the words `merchant`, `merchandise`, and `commerce`‚Äîall derived from the Roman god Mercury‚Äîto walk through byte-level BPE and see how it naturally learns meaningful subwords while completely eliminating out-of-vocabulary (OOV) issues.

### Base Vocabulary

Unlike classic BPE, GPT-2‚Äìstyle byte-level BPE operates directly on raw UTF-8 bytes, not characters or words. It starts from raw bytes. A byte is an 8-bit value, so it can represent exactly:

```css
256 possible tokens: 0, 1, 2, ..., 255
```

GPT-2‚Äôs base vocabulary contains one token for each of these 256 byte values. 



**Example 1: ASCII Text (`merchant`)**

Consider the word `merchant`, In UTF-8, each character is encoded as a single byte:

| Character | UTF-8 Byte (Decimal) | Byte Token |
| --------- | -------------------- | ---------- |
| `m`       | 109                  | token_109  |
| `e`       | 101                  | token_101  |
| `r`       | 114                  | token_114  |
| `c`       | 99                   | token_99   |
| `h`       | 104                  | token_104  |
| `a`       | 97                   | token_97   |
| `n`       | 110                  | token_110  |
| `t`       | 116                  | token_116  |

Before any BPE merges, the tokenizer represents `merchant` as:

```css
[109, 101, 114, 99, 104, 97, 110, 116]
```

Each number is already a valid token because every byte value is in the vocabulary.



**Example 2: Whitespace Is Also a Byte**

Whitespace is not special-cased‚Äîit‚Äôs just another byte.

| Character   | UTF-8 Byte |
| ----------- | ---------- |
| space (` `) | 32         |

For readability, GPT-style tokenizers often display byte `32` as:

```css
ƒ†
```

The string `  merchant` is represented as:

```css
ƒ†merchant
```

So the tokenizer encodes the string `  merchant` as:

```css
[32, 109, 101, 114, 99, 104, 97, 110, 116]
```



**Example 3: Non-ASCII Unicode Text (Why Bytes Matter)**

Consider the character:

```css
√©
```

In UTF-8, this is not one byte ‚Äî it‚Äôs two bytes:

| Character | UTF-8 Bytes (Hex) | Bytes (Decimal) |
| --------- | ----------------- | --------------- |
| `√©`       | `C3 A9`           | 195, 169        |

Because the base vocabulary includes all 256 byte values, the tokenizer can represent `√©` as:

```css
[195, 169]
```



**Example 4: Emoji (Sitll Works)**

Emoji are just bytes too.

```css
üî•
```

UTF-8 bytes:

```css
F0 9F 94 A5  ‚Üí  [240, 159, 148, 165]
```

Each of these byte values is already in the base vocabulary.



#### Why This Design Is Powerful

Because the base vocabulary includes all 256 byte values:

- Any text can be represented (ASCII, Unicode, emoji, code, binary)
- There are zero out-of-vocabulary tokens
- Tokenization is lossless and reversible
- BPE merges can focus purely on compression and structure learning



### Step-by-Step Algorithm Walkthrough

Next, we walk through **byte-level BPE** using the following training corpus:

```css
merchant merchandise commerce
```

Our goal is to show how BPE can automatically discover a **meaningful subword**, such as `merc`, **purely from statistics**‚Äîwithout any linguistic rules or prior knowledge.



#### Step 0: Byte-Level Initialization

Each word is first converted into its raw UTF-8 byte representation. For readability, we display bytes as characters, where the special symbol `ƒ†` denotes a leading whitespace byte (decimal value **32**).

| Word        | Byte-Level Tokens         | Token IDs                                                   |
| ----------- | ------------------------- | ----------------------------------------------------------- |
| merchant    | `m e r c h a n t`         | `[109, 101, 114, 99, 104, 97, 110, 116]`                    |
| merchandise | `ƒ† m e r c h a n d i s e` | `[32, 109, 101, 114, 99, 104, 97, 110, 100, 105, 115, 101]` |
| commerce    | `ƒ† c o m m e r c e`       | `[32, 99, 111, 109, 109, 101, 114, 99, 101]`                |

At this stage:

- The vocabulary consists of exactly 256 byte values
- Each byte (character) is treated as an independent token
- No subwords, words, or linguistic structure exist yet

From here, BPE will iteratively merge the most frequent adjacent byte pairs, gradually forming longer and more meaningful subword units.



#### Step 1: Identify the Most Frequent Byte Pair

The tokenizer scans the entire corpus and counts the frequencies of all adjacent byte pairs. 

Across these words, the most frequent byte is:

```css
(m, e)
```

 making it the most frequent candidate for merging.



#### Step 2: Merge and Assign a New Token ID

The most frequent byte pair is merged into a new token and assigned the next available token ID.

```css
(m, e) ‚Üí me     (ID = 256)
```

The vocabulary is updated accordingly:

| Character | UTF-8 Byte (Decimal) | Byte Token |
| --------- | -------------------- | ---------- |
| `m`       | 109                  | token_109  |
| `e`       | 101                  | token_101  |
| `r`       | 114                  | token_114  |
| `c`       | 99                   | token_99   |
| `h`       | 104                  | token_104  |
| `a`       | 97                   | token_97   |
| `n`       | 110                  | token_110  |
| `t`       | 116                  | token_116  |
| ...       | ...                  | ...        |
| `me`      | 256                  | token_256  |

All occurrences of `(e, r)` in the corpus are then replaced with the new token:

| Word        | Byte-Level Tokens        | Token IDs                                              |
| ----------- | ------------------------ | ------------------------------------------------------ |
| merchant    | `me r c h a n t`         | `[256, 114, 99, 104, 97, 110, 116]`                    |
| merchandise | `ƒ† me r c h a n d i s e` | `[32, 256, 114, 99, 104, 97, 110, 100, 105, 115, 101]` |
| commerce    | `ƒ† c o m me r c e`       | `[32, 99, 111, 109, 256, 114, 99, 101]`                |



#### Step 3: Next Iterations ‚Äî Recount and Merge

After updating the corpus, the tokenizer recomputes byte-pair frequencies.

The next most frequent pair is:

```css
(me, r)
```

This pair is merged and assigned a new token ID:

```css
(me, r) ‚Üí mer     (ID = 257)
```

| Word        | Byte-Level Tokens       | Token IDs                                         |
| ----------- | ----------------------- | ------------------------------------------------- |
| merchant    | `mer c h a n t`         | `[257, 99, 104, 97, 110, 116]`                    |
| merchandise | `ƒ† mer c h a n d i s e` | `[32, 257, 99, 104, 97, 110, 100, 105, 115, 101]` |
| commerce    | `ƒ† c o m mer c e`       | `[32, 99, 111, 109, 257, 99, 101]`                |

Recounting again reveals another frequent pair:

```css
(mer, c)
```

Merging yields:

```css
(mer, c) ‚Üí merc     (ID = 258)
```

| Word        | Byte-Level Tokens      | Token IDs                                     |
| ----------- | ---------------------- | --------------------------------------------- |
| merchant    | `merc h a n t`         | `[258, 104, 97, 110, 116]`                    |
| merchandise | `ƒ† merc h a n d i s e` | `[32, 258, 104, 97, 110, 100, 105, 115, 101]` |
| commerce    | `ƒ† c o m merc e`       | `[32, 99, 111, 109, 258, 99, 101]`            |



At this point, three new tokens‚Äî`me`, `mer`, and `merc`‚Äîhave been added to the vocabulary:

| Token  | Token ID |
| ------ | -------- |
| `me`   | 256      |
| `mer`  | 257      |
| `merc` | 258      |

Without any explicit linguistic rules, word boundaries, or morphological supervision, byte-level BPE naturally discovers the meaningful subword **`merc`**. This structure emerges purely from frequency-driven byte-pair merges, demonstrating how semantic units can arise from compression alone.



#### Next Steps: Repeat Until the Vocabulary Is Full

Steps 1, 2, and 3 are repeated iteratively. At each iteration, the tokenizer:

1. Recounts the frequencies of all adjacent byte pairs
2. Merges the most frequent pair into a new token

This loop continues until either
 (i) a predefined vocabulary size is reached, or
 (ii) no remaining merges yield meaningful compression.



For GPT-2, the final vocabulary contains **50,257 tokens**, composed of:

- 256 base byte tokens
- 1 special token (`<endoftext>`)
- 50K learned BPE merge tokens

> Unlike classic BPE, GPT-2 does **not** stop when byte pairs occur only once. Instead, it continues merging until the vocabulary size limit is reached.



### The Encoding and Decoding of Byte Pair Encoding

In our toy example, the byte-level BPE tokenizer is trained on the small corpus 

```css
merchant merchandise commerce
```

 The merge process terminates at a vocabulary size of **271**, at which point no additional merges yield meaningful or reusable subwords.

The learned merge tokens include:

| Token         | ID   |
| ------------- | ---- |
| `er`          | 256  |
| `mer`         | 257  |
| `merc`        | 258  |
| `merch`       | 259  |
| `mercha`      | 260  |
| `merchan`     | 261  |
| `merchant`    | 262  |
| `merchand`    | 263  |
| `merchandi`   | 264  |
| `merchandis`  | 265  |
| `merchandise` | 266  |
| `co`          | 267  |
| `com`         | 268  |
| `commerc`     | 269  |
| `commerce`    | 270  |



#### In-Vocabulary Example

**Encoding**

Now, let the trained BPE tokenizer encode the following sequence:

```css
merchant merchandise commerce 
```

The resulting token IDs are:

```css
[262, 32, 266, 32, 270]
```

Here, **token 32** corresponds to the whitespace character in the base byte vocabulary.

**Decoding**

Decoding the token sequence

```css
[262, 32, 266, 32, 270]
```

reconstructs the original text:

```css
merchant merchandise commerce
```



#### Out-of-Vocabulary Example

In Roman mythology, **Mercury** is also the god of speed and communication, responsible for delivering messages to Jupiter, the king of the gods. Astronomers later borrowed this name for the planet **Mercury**, which orbits the Sun once every 88 Earth days‚Äîmaking it the fastest-moving planet in the solar system.

The English word `mercurial`, meaning rapid and unpredictable changes of mood or behavior, traces its origin to the same mythological root. As a result, it naturally shares the subword `merc`.

Although the training corpus does **not** contain the word `mercurial`, the byte-level BPE tokenizer can still encode it by reusing learned subwords and falling back to base byte tokens.

Encoding `mercurial` yields the following token IDs:

```css
[258, 117, 114, 105, 97, 108]
```

These tokens correspond to:

| Token  | ID   |
| ------ | ---- |
| `merc` | 258  |
| `u`    | 117  |
| `r`    | 114  |
| `i`    | 105  |
| `a`    | 97   |
| `l`    | 108  |

Here, the tokenizer successfully reuses the learned subword `merc`, while the remaining suffix (`urial`) is decomposed into individual byte-level tokens from the base vocabulary.

This example illustrates a core strength of byte-level BPE: it never produces unknown tokens. Even when encountering unseen words, the tokenizer can always fall back to byte representations while still leveraging meaningful subword structure whenever possible.

 

## Takeaway

- **It works for any text:** Byte-level BPE starts from raw bytes, so it can handle English, other languages, emojis, and even weird symbols‚Äîwithout ever producing unknown tokens.
- **It learns useful pieces of words automatically:** By merging the most frequent byte pairs, BPE naturally discovers meaningful subwords like `merc` for trade- and speed-relevant words, instead of relying on hand-written linguistic rules.
- **One tokenizer, many models:** Once a BPE tokenizer is trained, other language models can reuse it directly, which is why many modern LLMs share the same or very similar tokenizers.
- **Simple but scalable:** Tokenization is kept simple and separate from the model itself, making byte-level BPE easy to train, easy to reuse, and reliable at large scale.



## Reference

If you‚Äôd like to reproduce the tokenization process step by step, you can find the complete implementation used in this blog here:

- [Byte Pair Encoding (BPE) Tokenizer ‚Äî From Scratch (GPT-2 Style)](https://github.com/fudonglin/blog_code_repository/tree/main/bpe)

For a more comprehensive BPE implementation, check out

- [MinBPE](https://github.com/karpathy/minbpe), a clean and well-documented repository by Andrej Karpathy.

If you prefer to experiment interactively:

- [Online GPT-2 Tokenizer Playground](https://tiktokenizer.vercel.app/?model=gpt2),  a great tool for visualizing how text is split into tokens in real time.



